{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85dd9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development of the predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6857ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('./data/develop_set_bayes_risk.csv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217ce9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration de-en-lang1=de,lang2=en\n",
      "Reusing dataset tatoeba (C:\\Users\\gerso\\.cache\\huggingface\\datasets\\tatoeba\\de-en-lang1=de,lang2=en\\0.0.0\\b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6678429f125f43f0b683ed2f8b5a770b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[276634, 20738, 2500, 2500, 5000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(58101, 512, padding_idx=58100)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=58101, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next load the model\n",
    "from utils.config_utils import parse_config\n",
    "parsed_config = parse_config('./configs/helsinki-taboeta-de-en.yml', pretrained=True)\n",
    "\n",
    "model = parsed_config[\"model\"]\n",
    "tokenizer = parsed_config[\"tokenizer\"]\n",
    "model = model.to(\"cuda\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7da8cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                source                     hypothesis  \\\n",
      "0  Ich spreche überhaupt kein Deutsch.   I can't speak German at all.   \n",
      "1  Ich spreche überhaupt kein Deutsch.  I cannot speak German at all.   \n",
      "2  Ich spreche überhaupt kein Deutsch.   I don't speak German at all.   \n",
      "3  Ich spreche überhaupt kein Deutsch.    I don't speak a French run.   \n",
      "4  Ich spreche überhaupt kein Deutsch.    I don't know German by all.   \n",
      "\n",
      "        avg       std  \n",
      "0  1.223635  1.223635  \n",
      "1  0.749200  0.749200  \n",
      "2  1.228001  1.228001  \n",
      "3  0.021646  0.021646  \n",
      "4  0.585859  0.585859  \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import ast\n",
    "# Next we need to create a proper dataset from it, first we split every columns\n",
    "columns = [\"source\", \"hypothesis\", \"avg\", \"std\"]\n",
    "expanded_dataset = pd.DataFrame(columns=columns)\n",
    "\n",
    "number_of_hypothesis = int((len(dataset.columns) - 1)/2)\n",
    "\n",
    "for i, row in dataset.iterrows():\n",
    "    source = row[\"source\"]\n",
    "    for j in range(number_of_hypothesis):\n",
    "        hypothesis = row[\"hypothesis_{}\".format(j)]\n",
    "        x = row[\"avg_std_{}\".format(j)]\n",
    "\n",
    "        if type(x) != float: # Bit of a hack but we have either a string containing a tuple or x is a float with value NaN\n",
    "            x = ast.literal_eval(x)\n",
    "            avg = float(x[0])\n",
    "            std = float(x[0])\n",
    "            expanded_dataset = expanded_dataset.append({\"source\": source, \"hypothesis\": hypothesis, \"avg\": avg, \"std\": std}, ignore_index=True)\n",
    "print(expanded_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169cae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we creat a dataset from it\n",
    "from datasets import tqdm, Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(expanded_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87da3776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481e4ea0d2d74617bdcbbc1dadc6ff7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Next we need to create a dataloader for it\n",
    "\n",
    "# First define the collate function\n",
    "\n",
    "def get_proprocess_function(tokenizer):\n",
    "    def preprocess_function(examples, tokenizer,):\n",
    "        source =examples[\"source\"]\n",
    "        targets = examples[\"hypothesis\"]\n",
    "        model_inputs = tokenizer(source,  truncation=True, )\n",
    "        # Setup the tokenizer for targets\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, truncation=True, )\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "        return model_inputs\n",
    "    return lambda examples: preprocess_function(examples, tokenizer)\n",
    "\n",
    "preprocess_function = get_proprocess_function(tokenizer)\n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ec4182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "import torch\n",
    "def get_collate_fn(model, tokenizer,):\n",
    "    data_collator = DataCollatorForSeq2Seq(model=model, tokenizer=tokenizer,\n",
    "                                           padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    keys = [\n",
    "        \"input_ids\",\n",
    "        \"attention_mask\",\n",
    "        \"labels\"\n",
    "    ]\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        \n",
    "        new_batch = [{k: s[k] for k in keys} for s in batch]\n",
    "        x_new = data_collator(new_batch)\n",
    "\n",
    "        sources = [s[\"source\"] for s in batch]\n",
    "        hypothesis = [s[\"hypothesis\"] for s in batch]\n",
    "        \n",
    "        #Group the averages and the standard deviations\n",
    "        avgs = torch.Tensor([s[\"avg\"] for s in batch])\n",
    "        std = torch.Tensor([s[\"std\"] for s in batch])\n",
    "        \n",
    "        return x_new, (sources, hypothesis), (avgs, std)\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "collate_fn = get_collate_fn(model, tokenizer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2421a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2b4008b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': tensor([[  149,  2670,    63,     5,    57,   783,   277,  4538,    18,     3,\n",
      "             0, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [ 2136,   109,  1680,     2,    44,   244,   718,   139,   567,  1830,\n",
      "         16977,     3,     0, 58100, 58100, 58100, 58100, 58100],\n",
      "        [  105, 26012,    51, 22740,     3,     0, 58100, 58100, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [17167,     2,   107,   334,    84,   260,  9497,   244,  9181,   524,\n",
      "          2209,   827,   104,   759,    31,    68,     0, 58100],\n",
      "        [ 2136,    29,     9,  2081, 26855,     5,     9,  1614,    10,    44,\n",
      "           810,   110,    89,     3,     0, 58100, 58100, 58100],\n",
      "        [   42,  4313,    74,     2,    78,  1762,    76,   106,  2695,  2187,\n",
      "             3,     0, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [   55,   644, 18751,    74,     3,     0, 58100, 58100, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [  105, 16412,   260,     2,   628,  2136, 12298,     5,  6811,    29,\n",
      "             3,     0, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [  105,   921,   467,  5617, 30119,     3,     0, 58100, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [ 2136, 26918,    96,     9, 29713,    21,   484,  5414,   788,  4268,\n",
      "           363,  1045,     3,     0, 58100, 58100, 58100, 58100],\n",
      "        [ 1667,    29,  5265,   174,   102,   600,    31,     0, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [ 2136,  4313,  2719,   716,  3453,    24,     3,     0, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [ 2136,    43,  3340,  2719,  5420,  1620,     2,   563,   110,  2970,\n",
      "          1051,     3,     0, 58100, 58100, 58100, 58100, 58100],\n",
      "        [  103,    29,  7550,    21, 10366,     3,     0, 58100, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [  119,  2502, 22911,    18,   363,  5023,  7412,  2274,     3,     0,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [ 2136,   109,    25,  2117,   590, 34498,     3,     0, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [   42,   109,    37,    57,  1018,   861, 36396,  6020,     3,     0,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [ 1667,   109,   449, 19004,    31,     0, 58100, 58100, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [  502,     9,  4003,    17,  3226,  3340,  2136,  1038,  1756,    49,\n",
      "          7250,    24,  1374,     3,     0, 58100, 58100, 58100],\n",
      "        [   42,    70,  3067,  1324,   145,   592, 20071,   245, 35882,     3,\n",
      "             0, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [ 2136, 27088,    44,  1335,     3,     0, 58100, 58100, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [  313,  1277,    11, 30374,   829,     3,     0, 58100, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [ 6332,   110,    11,  4003,  9060,   513,     2,  2426,   110,    96,\n",
      "          1659,     3,     0, 58100, 58100, 58100, 58100, 58100],\n",
      "        [  228,   324,  1256, 12671,   227,    76,   449,     3,     0, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [ 2136,    80,    28,   363,   679,   519, 10125,     3,     0, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [  323,  2069,  5824,   278, 13167,    68,     0, 58100, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [  246,  7766,   258, 29446,   486,     2,   176,   175,    65,    49,\n",
      "          7735,  1141,  2623,     3,     0, 58100, 58100, 58100],\n",
      "        [ 2136,  4313,  3873, 20998,    74,     3,     0, 58100, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [32971,    34,    42,  8380, 12317,    69, 19043,    15, 13571,    31,\n",
      "             0, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [ 9803,    29,     9,  1515,     2,    25,    84,  2373,  4180,    18,\n",
      "             3,     0, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [  182,   114,   185,    78,  3470,   296, 13080,     3,     0, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [   47, 18253,    45,  2987,    96,   327,  1073,     9,  1907,  4617,\n",
      "             2,    47,  1803,   223,   143, 17744,   221,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  441, 11021,    22,    46,   191,  7920, 37792,     6,     5,     4,\n",
      "           682,     3,     0,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2136,   466,    60,   206,   177,   339,   534,  1404,     3,     0,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [   38,    58,    66,  2553,    12, 16916,     3,     0,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 1104,     2,   406,   544,    38,   264, 32885,     5,   259,    31,\n",
      "             0,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2136,    19,     4,   517, 37890,     5,     4,   847,     8,   137,\n",
      "          5138,    35,     2,   675,     3,     0,  -100,  -100],\n",
      "        [  746,  7535,    52,   205,   473,    22,    82,  1770,    14, 26325,\n",
      "             3,     0,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  491,   840, 17686,     3,     0,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [   38, 11785,  1216,    62,    66,  2136,    19,     5,  4862,     3,\n",
      "             0,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [   38,    22,    82,   209,    12,   253,   153,    41,     3,     0,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2136,   371,  2273, 17308,   154,   226,  2950,   349,     4,   826,\n",
      "         11833, 27229,     3,     0,  -100,  -100,  -100,  -100],\n",
      "        [ 2266,  3271,    73,    43,    33,    41,    31,     0,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2136, 22925,  4741,   743,   473,    58, 13394,     3,     0,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2136,  2087,  4741,   153,   299,   137,   215,   152,     3,     0,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  131,    58,   950,    22,     6,  6640,     3,     0,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [   36,  2592,  8029,   136, 44372,  1047,     3,     0,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2136,  6259,     4,  2472,   421, 15064,     3,     0,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  746,  5059,   286, 35646,    67,     4,  3084,     3,     0,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2266, 30069,   303,    31,     0,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2136,   359,    12,   548,   154,  2415,   430,    12,     4,  2784,\n",
      "           349,  1604,     3,     0,  -100,  -100,  -100,  -100],\n",
      "        [  491,  1114,  2124,   653,   546,   125,  2097,     3,     0,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2136,  2374,     4,  1398,     3,     0,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  146,    22,   135,     4,  3134,  1590,     3,     0,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 1034,  1644,  1604,     2,   137,  1825,    12,  1659,     3,     0,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  236,   384,   473, 44108,   303,     3,     0,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2136,    73,  2053,   251,    23,   154,  1857,     3,     0,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [43808,   164,   128,  1391,     3,     0,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [   93, 17366,  5090,   150,   232,    56,    19,  3119,    12,  8232,\n",
      "             3,     0,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2136,  7535,  6243,  5629,   132,     3,     0,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  629,    41, 10622,  2189, 16045,    62, 12570, 16045,    31,     0,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [46288,    19,     4,  8437,    38, 10622,     3,     0,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  157,    72,   126,   219,  1976,   160,     3,     0,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [   47, 23231,  2234,   380,     6,  1439,   466,     4,   839,  4872,\n",
      "             2,    47,  9464,    41,    48, 32293,   221,     0]]), 'decoder_input_ids': tensor([[58100,   441, 11021,    22,    46,   191,  7920, 37792,     6,     5,\n",
      "             4,   682,     3,     0, 58100, 58100, 58100, 58100],\n",
      "        [58100,  2136,   466,    60,   206,   177,   339,   534,  1404,     3,\n",
      "             0, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,    38,    58,    66,  2553,    12, 16916,     3,     0, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,  1104,     2,   406,   544,    38,   264, 32885,     5,   259,\n",
      "            31,     0, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,  2136,    19,     4,   517, 37890,     5,     4,   847,     8,\n",
      "           137,  5138,    35,     2,   675,     3,     0, 58100],\n",
      "        [58100,   746,  7535,    52,   205,   473,    22,    82,  1770,    14,\n",
      "         26325,     3,     0, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,   491,   840, 17686,     3,     0, 58100, 58100, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,    38, 11785,  1216,    62,    66,  2136,    19,     5,  4862,\n",
      "             3,     0, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,    38,    22,    82,   209,    12,   253,   153,    41,     3,\n",
      "             0, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,  2136,   371,  2273, 17308,   154,   226,  2950,   349,     4,\n",
      "           826, 11833, 27229,     3,     0, 58100, 58100, 58100],\n",
      "        [58100,  2266,  3271,    73,    43,    33,    41,    31,     0, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,  2136, 22925,  4741,   743,   473,    58, 13394,     3,     0,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,  2136,  2087,  4741,   153,   299,   137,   215,   152,     3,\n",
      "             0, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,   131,    58,   950,    22,     6,  6640,     3,     0, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,    36,  2592,  8029,   136, 44372,  1047,     3,     0, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,  2136,  6259,     4,  2472,   421, 15064,     3,     0, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,   746,  5059,   286, 35646,    67,     4,  3084,     3,     0,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,  2266, 30069,   303,    31,     0, 58100, 58100, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,  2136,   359,    12,   548,   154,  2415,   430,    12,     4,\n",
      "          2784,   349,  1604,     3,     0, 58100, 58100, 58100],\n",
      "        [58100,   491,  1114,  2124,   653,   546,   125,  2097,     3,     0,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,  2136,  2374,     4,  1398,     3,     0, 58100, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,   146,    22,   135,     4,  3134,  1590,     3,     0, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,  1034,  1644,  1604,     2,   137,  1825,    12,  1659,     3,\n",
      "             0, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,   236,   384,   473, 44108,   303,     3,     0, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,  2136,    73,  2053,   251,    23,   154,  1857,     3,     0,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100, 43808,   164,   128,  1391,     3,     0, 58100, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,    93, 17366,  5090,   150,   232,    56,    19,  3119,    12,\n",
      "          8232,     3,     0, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,  2136,  7535,  6243,  5629,   132,     3,     0, 58100, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,   629,    41, 10622,  2189, 16045,    62, 12570, 16045,    31,\n",
      "             0, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100, 46288,    19,     4,  8437,    38, 10622,     3,     0, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,   157,    72,   126,   219,  1976,   160,     3,     0, 58100,\n",
      "         58100, 58100, 58100, 58100, 58100, 58100, 58100, 58100],\n",
      "        [58100,    47, 23231,  2234,   380,     6,  1439,   466,     4,   839,\n",
      "          4872,     2,    47,  9464,    41,    48, 32293,   221]])}, (['Es befinden sich in dem Zimmer keine Tische.', 'Tom hat gesagt, das hier würde nur drei Stunden dauern.', 'Ich durfte nicht singen.', 'Mist, wie habe ich mich bloß hier hineinmanövriert?!', 'Tom ist der beste Tänzer in der Gruppe und das weiß er auch.', 'Sie sah aus, als hätte sie einen Geist gesehen.', 'Die sehen großartig aus.', 'Ich frage mich, ob Tom echt in Australien ist.', 'Ich möchte dich näher kennenlernen.', 'Tom heiratete nach der Scheidung von seiner dritten Frau erneut seine erste.', 'Wer ist sonst noch bei dir?', 'Tom sah Maria beim Essen zu.', 'Tom belog Maria bezüglich dessen, wo er gewesen sei.', 'Das ist Schnee von gestern.', 'Der Boden bewahrte seine Reichhaltigkeit.', 'Tom hat den ganzen Tag geschlafen.', 'Sie hat auf dem Markt einige Äpfel verkauft.', 'Wer hat ihn geschlagen?', 'Nach der Schule pflog Tom seinem Vater im Laden zu helfen.', 'Sie sind zehn Tage vor unserer Feier abgereist.', 'Tom brauchte das Geld.', 'Du bist die Süßeste.', 'Nachdem er die Schule beendet hatte, ging er nach London.', 'Und dann küsste sie ihn.', 'Tom wird für seine Arbeit gut bezahlt.', 'Versprich mir Eines!', 'Ein Fahrrad verrostet, wenn man es im Regen stehen lässt.', 'Tom sah recht müde aus.', 'Mögen Sie lieber weißen oder braunen Reis?', 'Fußball ist der Sport, den ich bevorzuge.', 'Wir haben mehr als genug Zeit übrig.', '\"Meiner Meinung nach\", sagte der kleine Bruder, \"hast du Unrecht.\"'], [\"There aren't noUn necklaces in the room.\", 'Tom said this would only take three hours.', 'I was not allowed to sing.', 'Oh, how did I just sneak in here?', 'Tom is the best dancer in the group and he knows that, too.', \"She looked as if she'd seen a ghost.\", 'They look systematic.', 'I wonder whether or not Tom is in Australia.', \"I'd like to know about you.\", 'Tom remarried his first woman after the malpir gay.', 'Who else will be with you?', 'Tom alike Mary while she was eating.', 'Tom told Mary about where he had been.', \"It was today's snow.\", 'The groundHer its rudeness.', 'Tom spent the entire day sleeping.', 'She sold some apples at the fair.', 'Who beaten him?', 'Tom used to help his father go to the shop after school.', 'They left ten days during our party.', 'Tom needed the money.', \"You're the cutest.\", 'After complete school, he went to London.', 'And then she kissed him.', 'Tom will pay well for his job.', 'Promise me one thing.', 'A bicycle Old up when it is expected to rain.', 'Tom looked pretty sleepy.', 'Do you prefer white rice or brown rice?', 'Soccer is the sport I prefer.', 'We have more than enough time.', '\" Wor populationtos,\" said the little brother, \"Our you are mistaken.\"']), (tensor([-0.3144,  1.0597,  1.3308, -0.1017,  0.9572,  1.1102, -0.1164,  0.7841,\n",
      "         0.3744, -0.3883,  0.6282, -0.2808,  0.8301,  0.0046, -0.8048,  0.2007,\n",
      "         0.6526,  1.0018,  0.3252,  0.8684,  1.6935, -0.1751,  0.7712,  1.9084,\n",
      "         0.6913,  0.9830, -0.4718,  0.9736,  1.2081,  1.0273,  1.5596, -0.3766]), tensor([-0.3144,  1.0597,  1.3308, -0.1017,  0.9572,  1.1102, -0.1164,  0.7841,\n",
      "         0.3744, -0.3883,  0.6282, -0.2808,  0.8301,  0.0046, -0.8048,  0.2007,\n",
      "         0.6526,  1.0018,  0.3252,  0.8684,  1.6935, -0.1751,  0.7712,  1.9084,\n",
      "         0.6913,  0.9830, -0.4718,  0.9736,  1.2081,  1.0273,  1.5596, -0.3766])))\n"
     ]
    }
   ],
   "source": [
    "#Check the output of 1 batch\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b505516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarianMTModel(\n",
      "  (model): MarianModel(\n",
      "    (shared): Embedding(58101, 512, padding_idx=58100)\n",
      "    (encoder): MarianEncoder(\n",
      "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
      "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
      "      (layers): ModuleList(\n",
      "        (0): MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): MarianDecoder(\n",
      "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
      "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
      "      (layers): ModuleList(\n",
      "        (0): MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=58101, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb956585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we create the predictive model \n",
    "from torch import nn\n",
    "\n",
    "def avg_pooling(hidden_state, attention_mask):\n",
    "    attention_hidden_state = hidden_state * attention_mask.unsqueeze(dim=-1)\n",
    "    sum_hidden_state = torch.sum(attention_hidden_state, dim=1)\n",
    "    n = torch.sum(attention_mask.unsqueeze(dim=-1), dim=1)\n",
    "    avg_hidden_state = sum_hidden_state / n\n",
    "    return avg_hidden_state\n",
    "\n",
    "class PredictiveModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, NMT_model, embedding_size=512):\n",
    "        super().__init__()\n",
    "        self.NMT_model = NMT_model\n",
    "        # Then we create a two layer feed forward network\n",
    "\n",
    "        # NMT model does not require a gradient\n",
    "        self.NMT_model.requires_grad = False\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(nn.Linear(512 * 2, 512), torch.nn.SiLU() , nn.Linear(512, 2)) \n",
    "        \n",
    "        self.softplus = nn.Softplus()\n",
    "        self.padding_id = -100\n",
    "        \n",
    "    def forward(self, input_ids,  attention_mask=None, labels=None, decoder_input_ids=None):\n",
    "        nmt_out = self.NMT_model.forward(input_ids=input_ids, attention_mask=attention_mask, labels=labels, decoder_input_ids=decoder_input_ids, output_hidden_states=True, output_attentions=True)\n",
    "        encoder_last_hidden_state = nmt_out[\"encoder_last_hidden_state\"]\n",
    "        decoder_last_hidden_state = nmt_out[\"decoder_hidden_states\"][-1]\n",
    "        logits = nmt_out[\"logits\"]\n",
    "       \n",
    "        # Next perform average pooling\n",
    "        # first apply attention_mask to encoder_last_hidden_state\n",
    "  \n",
    "        avg_encoder_hidden_state = avg_pooling(encoder_last_hidden_state, attention_mask)\n",
    "        # Then devide \n",
    "        \n",
    "        attention_mask_decoder = (self.padding_id != labels).long()\n",
    "        avg_decoder_hidden_state = avg_pooling(decoder_last_hidden_state, attention_mask_decoder)\n",
    " \n",
    "        # Concat the two\n",
    "        hidden_states_concat = torch.cat([avg_encoder_hidden_state, avg_decoder_hidden_state], dim=-1)\n",
    "        \n",
    "        x = self.linear_layers(hidden_states_concat)\n",
    "        #Next we apply softplus to the second part of x\n",
    "        avg = x[:,0:1]\n",
    "        std = x[:, 1:]\n",
    "\n",
    "        std = self.softplus(std)\n",
    "        return avg, std\n",
    "predictive_model = PredictiveModel(model).to(\"cuda\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bede0a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cdc262c1ee4cda99dc49244acfaa59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.173\n",
      "[1,    40] loss: 0.503\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8457882272d4b9caaeeb7666fe1db53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,    20] loss: 0.286\n",
      "[2,    40] loss: 0.761\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e2108f2cd04dbcb0bec078a614eec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,    20] loss: 0.705\n",
      "[3,    40] loss: 0.545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4350db4fc8cc4763b8dab169027b8cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,    20] loss: 1.049\n",
      "[4,    40] loss: 0.804\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48916a853124d41b2272748ffeec8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5,    20] loss: 0.328\n",
      "[5,    40] loss: 0.359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58c13eddf1f4f4e9711a0e4827c5cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,    20] loss: 0.348\n",
      "[6,    40] loss: 0.252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6496b45e24de475cb3642d08564c9885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7,    20] loss: -0.006\n",
      "[7,    40] loss: 0.085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6a0f111e534fcfbd2d7954ff165337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8,    20] loss: -0.013\n",
      "[8,    40] loss: -0.078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadfcacc6ca64fb19ddd9785041bfff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9,    20] loss: 0.375\n",
      "[9,    40] loss: -0.019\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6a78547af247b8bb96d6b5fe6bc0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,    20] loss: -0.059\n",
      "[10,    40] loss: 0.038\n"
     ]
    }
   ],
   "source": [
    "### Lastly we can start the training\n",
    "import torch.optim as optim\n",
    "criterion = nn.GaussianNLLLoss()\n",
    "\n",
    "predictive_model = PredictiveModel(model).to(\"cuda\")  \n",
    "optimizer = optim.Adam(predictive_model.linear_layers.parameters(), lr=0.0001,)\n",
    "n_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    i = 0\n",
    "    running_loss = 0.0\n",
    "    for x, (sources, targets), (avg, std) in tqdm(dataloader):\n",
    "        x = {k: v.to(\"cuda\") for k, v in x.items()}\n",
    "        (predicted_avg, predicted_std) = predictive_model.forward(**x)\n",
    "        \n",
    "        avg = avg.to(\"cuda\")\n",
    "        predicted_avg = predicted_avg.flatten()\n",
    "        \n",
    "        var = predicted_std.flatten()       \n",
    "        loss = criterion(predicted_avg, avg, var=var)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20:.3f}')\n",
    "\n",
    "            running_loss = 0.0\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2ccbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
