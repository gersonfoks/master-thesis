# Copies the structure of the NTM config, is used for loading the NMT model

dataset:
  dir: 'predictive/tatoeba-de-en/data/raw/'
  sampling_method: 'ancestral'
  n_hypotheses: 10
  n_references: 100
  repeated_indices: False
batch_size: 8
accumulate_grad_batches: 16 # Effective batch size of 128


save_loc: 'predictive/tatoeba-de-en/models/prompt-tuning/'

model_config:
  lr: 0.0001
  weight_decay: 0.0
  n_encoder_prompts: 0
  n_start_decoder_prompts: 5
  n_end_decoder_prompts: 5
  loss_function: 'MSE'
  preprocess_type: 'full'
  model_type: 'prompt-tuning'
  dropout: 0.0
  nmt_model:
    model:
      name: 'Helsinki-NLP/opus-mt-de-en'
      checkpoint: 'NMT/tatoeba-de-en/model'
      type: 'MarianMT'
  activation_function: 'silu'
  predictive_layers:
    - 512
    - 256
    - 128
    - 64
    - 32
    - 1
  optimizer: "adam"



