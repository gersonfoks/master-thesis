# Copies the structure of the NTM config, is used for loading the NMT model

dataset:
  preprocess_dir: 'predictive/tatoeba-de-en/data/preprocessed/'
  sampling_method: 'ancestral'
  n_hypotheses: 100
  n_references: 1000
  repeated_indices: False
batch_size: 128


model_config:
  lr: 1.4815897489348615e-05
  weight_decay: 1.001726887106992e-06
  loss_function: 'gaussian-mixture'
  preprocess_type: 'full'
  model_type: 'cross_attention'
  cross_attention_layers:
    n_heads: 4
    hidden_dim: 512
    cross_attention:
      -
        - decoder_hidden_state_-1
        - encoder_hidden_state_-1
      -
        - encoder_hidden_state_-1
        - decoder_hidden_state_-1
      -
        - decoder_hidden_state_-1
        - decoder_hidden_state_-1
      -
        - encoder_hidden_state_-1
        - encoder_hidden_state_-1
  query_layers:
    n_queries: 2
    n_heads: 4
    hidden_dim: 512
  dropout: 0.6712665889822903
  features:
    decoder_hidden_state: # Which hidden states to use
      - -1
    encoder_hidden_state: # Which hidden states to use
      - -1
  nmt_model:
    model:
      name: 'Helsinki-NLP/opus-mt-de-en'
      checkpoint: 'NMT/tatoeba-de-en/model'
      type: 'MarianMT'
  activation_function: 'silu'
  predictive_layers:
    - 4096 # n_queries * n_features * 512 (hidden size) (Read the n_features from features)
    - 2048
    - 1024
    - 512
    - 6 # Mixture of 2 times loc, 2 times scale 2 choices
  optimizer: "adam"
  n_mixtures: 2


