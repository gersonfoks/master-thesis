# Copies the structure of the NTM config, is used for loading the NMT model

dataset:
  preprocess_dir: 'predictive/tatoeba-de-en/data/preprocessed/'
  sampling_method: 'ancestral'
  n_hypotheses: 10
  n_references: 100
  repeated_indices: False
batch_size: 128


save_loc: 'predictive/tatoeba-de-en/models/MSE-attention/'

model_config:
  lr: 1.4815897489348615e-05
  weight_decay: 1.001726887106992e-06
  loss_function: 'MSE'
  preprocess_type: 'full'
  model_type: 'cross_attention'
  cross_attention_layers:
    n_heads: 4
    hidden_dim: 512
    cross_attention:
      - - decoder_hidden_state_-1
        - decoder_hidden_state_-1
  query_layers:
    n_queries: 2
    n_heads: 4
    hidden_dim: 512
  dropout: 0.6712665889822903
  features:
    decoder_hidden_state: # Which hidden states to use
      - -1
    encoder_hidden_state: # Which hidden states to use
      - -1
  nmt_model:
    model:
      name: 'Helsinki-NLP/opus-mt-de-en'
      checkpoint: 'NMT/tatoeba-de-en/model'
      type: 'MarianMT'
  activation_function: 'silu'
  predictive_layers:
    - 1024 # n_queries * n_features * 512 (hidden size) (Read the n_features from features)
    - 512
    - 256
    - 128
    - 1
  optimizer: "adam_with_plateau"
  patience: 3



