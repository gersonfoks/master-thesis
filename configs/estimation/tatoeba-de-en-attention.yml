# Copies the structure of the NTM config, is used for loading the NMT model

dataset:
  base_dir: 'FBR/NMT/tatoeba-de-en/data'
  preprocess_dir: 'FBR/predictive/preprocessed/tatoeba-de-en/COMET/last_layers'
  sampling_method: 'ancestral'
  n_hypothesis: 10
  n_references: 100
batch_size: 64


model_config:
  lr: 0.0001
  weight_decay: 0.0
  loss_function: 'MSE'
  preprocess_type: 'full'
  model_type: 'self_attention'
  attention_layers:
    n_heads: 8
    hidden_dim: 512
  query_layers:
    n_queries: 2
    n_heads: 8
    hidden_dim: 512
  dropout: 0.25
  features:
    decoder_hidden_state: # Which hidden states to use
      - -1
      - -2
  nmt_model:
    model:
      name: 'Helsinki-NLP/opus-mt-de-en'
      checkpoint: 'NMT/tatoeba-de-en/model'
      type: 'MarianMT'
  activation_function: 'silu'
  predictive_layers:
    - 2048 # n_queries * n_features * 512 (hidden size) (Read the n_features from features)
    - 2048
    - 1024
    - 512
    - 512
    - 256
    - 128
    - 64
    - 1
  optimizer: "adam"


