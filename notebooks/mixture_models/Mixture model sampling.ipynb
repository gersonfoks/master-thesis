{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "179f3349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some setup code for imports\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from custom_datasets.BayesRiskDatasetLoader import BayesRiskDatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0e39d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First we will load the test set with the calculated scores.\n",
    "dataset_loader = BayesRiskDatasetLoader(\"validation_predictive\", n_hypotheses=100, n_references=1000, sampling_method='ancestral')\n",
    "dataset = dataset_loader.load(type=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2667cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerso\\FBR\\NMT/tatoeba-de-en/model\n",
      "1.001726887106992e-06\n",
      "using a mixture model\n"
     ]
    }
   ],
   "source": [
    "### Next up we will read the trained model and calculate the score given by the heads\n",
    "from models.pl_predictive.PLPredictiveModelFactory import PLPredictiveModelFactory\n",
    "path = \"C:/Users/gerso/FBR/predictive/tatoeba-de-en/models/mixture_model_2/\"\n",
    "model, factory = PLPredictiveModelFactory.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87fef453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.distributions as td\n",
    "\n",
    "from models.MBR_model.BaseMBRModel import BaseMBRModel\n",
    "from utils.translation_model_utils import batch_sample, batch\n",
    "\n",
    "\n",
    "class GaussianMixtureMBRModel(BaseMBRModel):\n",
    "    '''\n",
    "    Model wraps the NMT model\n",
    "    '''\n",
    "\n",
    "    def __init__(self, predictive_model, device=\"cuda\", sample_size=1000):\n",
    "        super().__init__(predictive_model)\n",
    "        self.device_name = device\n",
    "        self.predictive_model = predictive_model.to(self.device_name)\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def forward(self, source, n_samples_per_source=256, batch_size=16, ):\n",
    "        hypotheses = batch_sample(self.predictive_model.nmt_model, self.predictive_model.tokenizer, [source],\n",
    "                                  n_samples=n_samples_per_source, )\n",
    "\n",
    "        return self.get_best(source, hypotheses)\n",
    "\n",
    "    def get_scores(self, sources, samples, batch_size=16):\n",
    "\n",
    "        model_out = self.get_model_out(sources, samples, batch_size=batch_size)\n",
    "\n",
    "        scores = self.model_out_to_risk(model_out)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def get_model_out(self, sources, samples, batch_size=16):\n",
    "        result = {}\n",
    "        for x, y in zip(batch(sources, n=batch_size), batch(samples, n=batch_size)):\n",
    "            model_out = self.predictive_model.predict(x, y)\n",
    "            result = self.add_model_out_to_result(result, model_out)\n",
    "        result = {k: torch.tensor(v) for k, v in result.items()}\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_best(self, source, hypotheses, batch_size=16):\n",
    "        sources = [source] * len(hypotheses)\n",
    "\n",
    "        scores = self.get_scores(sources, hypotheses, batch_size=batch_size)\n",
    "\n",
    "        best_index = np.argmax(scores)\n",
    "\n",
    "        best = hypotheses[best_index]\n",
    "\n",
    "        return best\n",
    "\n",
    "    def model_out_to_risk(self, model_out):\n",
    "\n",
    "        mixture = self.get_mixture(model_out[\"loc\"], model_out[\"scale\"], model_out[\"logits\"])\n",
    "\n",
    "        sample_scores = mixture.sample()\n",
    "\n",
    "        mean = sample_scores.mean(-1)\n",
    "        return mean\n",
    "\n",
    "    def get_mean(self, sources, samples, batch_size=16):\n",
    "        model_out = self.get_model_out(sources, samples, batch_size=batch_size)\n",
    "        mixture = self.get_mixture(model_out[\"loc\"], model_out[\"scale\"], model_out[\"logits\"])\n",
    "        \n",
    "        sample_scores = mixture.sample((100000,))\n",
    "        print(sample_scores.shape)\n",
    "        mean = sample_scores.mean(0)\n",
    "\n",
    "        return mean\n",
    "\n",
    "\n",
    "    def add_model_out_to_result(self, result, model_out):\n",
    "        if result == {}:\n",
    "            result[\"loc\"] = []\n",
    "            result[\"scale\"] = []\n",
    "            result[\"logits\"] = []\n",
    "        result['loc'] += list(model_out[0].cpu().numpy())\n",
    "        result['scale'] += list(model_out[1].cpu().numpy())\n",
    "        result['logits'] += list(model_out[2].cpu().numpy())\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_mixture(self, loc, scale, logits):\n",
    "        components = self.make_components(loc, scale)\n",
    "        mixture = td.MixtureSameFamily(td.Categorical(logits=logits), components)\n",
    "        \n",
    "        return mixture\n",
    "\n",
    "    def make_components(self, loc, scale):\n",
    "        sample_size = self.sample_size\n",
    "        shape = loc.shape\n",
    "        loc = loc.unsqueeze(-1)\n",
    "        scale = scale.unsqueeze(-1)\n",
    "        return td.Independent(td.Normal(loc=loc, scale=scale), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "da2988c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model = GaussianMixtureMBRModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0eb69e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                                                                                                                                     | 5/2500 [00:00<00:51, 48.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 2, 1])\n",
      "torch.Size([10000, 2, 1])\n",
      "torch.Size([10000, 2, 1])\n",
      "torch.Size([10000, 2, 1])\n",
      "torch.Size([10000, 2, 1])\n",
      "torch.Size([10000, 2, 1])\n",
      "torch.Size([10000, 2, 1])\n",
      "torch.Size([10000, 2, 1])\n",
      "torch.Size([10000, 2, 1])\n",
      "torch.Size([10000, 2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▊                                                                                                                                                                                                    | 11/2500 [00:00<00:57, 42.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 2, 1])\n",
      "torch.Size([10000, 2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Here we get the predicted loc and scale for each sentence:\n",
    "from tqdm import tqdm\n",
    "all_model_out = []\n",
    "for i, row in tqdm(dataset.data.iterrows(), total=dataset.data.shape[0]):\n",
    "    source = row[\"source\"]\n",
    "    hypotheses = list(row[\"hypotheses\"])[:2]\n",
    "\n",
    "    model_out = wrapped_model.get_mean([source]* len(hypotheses), hypotheses)\n",
    "    all_model_out.append(model_out)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later\n",
    "\n",
    "## Next up we check if we are optimistic or pessimistic.\n",
    "\n",
    "def calc_opt_statistics(predicted_scores, reference_scores):\n",
    "    opt_count = 0\n",
    "    pes_count = 0\n",
    "    opt_sum = 0\n",
    "    pes_sum = 0\n",
    "    for pred_score, ref_score in zip(predicted_scores, reference_scores):\n",
    "        larger = pred_score >= ref_score\n",
    "        smaller = ref_score > pred_score\n",
    "        opt_count += sum(larger)\n",
    "        pes_count += sum(smaller)\n",
    "        opt_sum += sum((pred_score - ref_score) * larger)\n",
    "        pes_sum += sum((ref_score - pred_score) * smaller)\n",
    "    total = opt_count + pes_count\n",
    "    opt_percentage = opt_count/total\n",
    "    pes_percentage = pes_count/total\n",
    "    opt_avg = opt_sum/opt_count\n",
    "    pes_avg = pes_sum/pes_count\n",
    "        \n",
    "    return {\"opt_count\": opt_count, \"pes_count\": pes_count, \n",
    "            \"opt_sum\": opt_sum, \"pes_sum\": pes_sum,\n",
    "            \"opt_percentage\": opt_percentage,\n",
    "            \"pes_percentage\": pes_percentage,\n",
    "            \"opt_avg\": opt_avg,\n",
    "            \"pes_avg\": pes_avg,\n",
    "           }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
