{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13eaabaf",
   "metadata": {},
   "source": [
    "In this notebook we will compare the MBR scores calculated with the monte-carlo estimates and those from the predictive model\n",
    "We will try to analyse failure points and think about how to tackle those.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some setup code for imports\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from custom_datasets.BayesRiskDatasetLoader import BayesRiskDatasetLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b53f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First we will load the validation set with the calculated scores.\n",
    "dataset_loader = BayesRiskDatasetLoader(\"validation_predictive\", n_hypotheses=100, n_references=1000, sampling_method='ancestral')\n",
    "validation_dataset = dataset_loader.load(type=\"pandas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_mbr_scores(entry):\n",
    "    scores = []\n",
    "    count = np.sum(entry[\"utilities_count\"])\n",
    "    for hyp, util in zip(entry[\"hypotheses\"], entry[\"utilities\"]):\n",
    "        score = np.sum(util * entry[\"utilities_count\"])/count\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9893fa84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8195cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Next up we will read the trained model and calculate the score given by the heads\n",
    "from models.pl_predictive.PLPredictiveModelFactory import PLPredictiveModelFactory\n",
    "\n",
    "model_name = \"student-t-3-repeated\"\n",
    "path = './{}/'.format(model_name)\n",
    "model_path = \"C:/Users/gerso/FBR/predictive/tatoeba-de-en/models/\"+ model_name + '/'\n",
    "model, factory = PLPredictiveModelFactory.load(model_path)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91494bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.MBR_model.GaussianMixtureMBRModel import GaussianMixtureMBRModel\n",
    "from models.MBR_model.StudentTMixtureMBRModel import StudentTMixtureMBRModel\n",
    "wrapped_model = StudentTMixtureMBRModel(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def get_repeated_utils(utilities, count):\n",
    "    \n",
    "    repeated_utils = []\n",
    "    for util in utilities:\n",
    "        \n",
    "        r = []\n",
    "        for u, c in zip(util, count):\n",
    "            r += [u] * c\n",
    "        repeated_utils.append(r)\n",
    "    return repeated_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def clean(s):\n",
    "    return s.replace('?', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40204ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_distributions(source, target, hypotheses, utilities, model, max_show=5, path='./validtion_imgs/'):\n",
    "    cleaned_source = clean(source)\n",
    "    cleaned_target = clean(target)\n",
    "    \n",
    "    sns.set_style(\"dark\")\n",
    "    \n",
    "    \n",
    "    samples = model.get_samples([source]*len(hypotheses), hypotheses, n_samples=1000)\n",
    "    \n",
    "    samples = samples.permute([1, 0, 2]).squeeze(-1).cpu().numpy().tolist()\n",
    " \n",
    "    \n",
    "    \n",
    "#     monte_carlo_1_sorted = monte_carlo_scores[sorted_indices][:max_show]\n",
    "#     predicted_scores_1_sorted = predicted_scores[sorted_indices][:max_show]\n",
    "#     hypotheses_sorted = hypotheses[sorted_indices][:max_show]\n",
    "    \n",
    "    utilities = utilities[:max_show]\n",
    "    samples = samples[:max_show]\n",
    "    hypotheses = hypotheses[:max_show]\n",
    "    data = {\n",
    "        \"Utility\": [],\n",
    "        \"Hypothesis\": [],\n",
    "        \"Samples\": [] # Either Monte-carlo or predictive\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for u, h in zip(utilities, hypotheses):\n",
    "        \n",
    "        data[\"Utility\"] += u\n",
    "        data[\"Hypothesis\"] += [h] * len(u)\n",
    "        data[\"Samples\"] += [\"Data\"] * len(u)\n",
    "    for s, h in zip(samples, hypotheses):\n",
    "        data[\"Utility\"] += s\n",
    "        data[\"Hypothesis\"] += [h] * len(s)\n",
    "        data[\"Samples\"] += [\"Model\"] * len(s)\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    g = sns.displot(df, x='Utility', col=\"Hypothesis\", row='Samples', facet_kws=dict(margin_titles=True))\n",
    "    title = 'Source: {} \\ntarget: {}'.format(source, target)\n",
    "    \n",
    "    g.fig.subplots_adjust(top=0.9) # adjust the Figure in rp\n",
    "    g.fig.suptitle(title)\n",
    "\n",
    "    def specs(x, **kwargs):\n",
    "        plt.axvline(x.mean(), c=\"red\", alpha=0.5, linestyle='--')\n",
    "        \n",
    "\n",
    "    g.map(specs, 'Utility')\n",
    "    fig = g.fig\n",
    "    \n",
    "    name = \"/{}.png\".format(cleaned_source)\n",
    "    save_file = str(path) + name \n",
    "    fig.savefig(save_file) \n",
    "    \n",
    "    \n",
    "    \n",
    "#     sns.distplot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# We want to create some plots for random samples\n",
    "def compare_random_samples(data, model, n_examples=5, seed=1, max_show=5, save=True, path='./'):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.choice(len(data), size=n_examples)\n",
    "    \n",
    "    for i in indices:\n",
    "        entry = data.iloc[i]\n",
    "        source = entry[\"source\"]\n",
    "        hypotheses = np.array(entry[\"hypotheses\"])\n",
    "        \n",
    "        source = entry[\"source\"]\n",
    "        target = entry[\"target\"]\n",
    "        hypotheses = entry[\"hypotheses\"].tolist()\n",
    "        utilities = get_repeated_utils(entry[\"utilities\"], entry[\"utilities_count\"])\n",
    "        plot_distributions(source, target, hypotheses, utilities, wrapped_model, path=path)\n",
    "        \n",
    "img_path_str =  path + 'validation_imgs/'\n",
    "img_path = Path(img_path_str)\n",
    "img_path.mkdir(parents=True, exist_ok=True)\n",
    "compare_random_samples(validation_dataset.data, wrapped_model, path=img_path)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f2498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Do the same for the training data:\n",
    "train_dataset_loader = BayesRiskDatasetLoader(\"train_predictive\", n_hypotheses=100, n_references=1000, sampling_method='ancestral')\n",
    "dataset_train = train_dataset_loader.load(type=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2605e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "img_path_str =  path + 'training_imgs/'\n",
    "img_path = Path(img_path_str)\n",
    "img_path.mkdir(parents=True, exist_ok=True)\n",
    "compare_random_samples(dataset_train.data, wrapped_model, path=img_path)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64366dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Next up we want to compute the MSE\n",
    "\n",
    "# First we get the data mean of each hypothesis\n",
    "\n",
    "\n",
    "# Then we get the predicted mean\n",
    "from tqdm import tqdm\n",
    "all_mbr_scores = []\n",
    "all_predicted_scores = []\n",
    "\n",
    "for row in tqdm(validation_dataset.data.iterrows(), total=2500):\n",
    "    entry = row[1]\n",
    "        \n",
    "    mbr_scores = calculate_mbr_scores(entry)\n",
    "\n",
    "    src = entry[\"source\"]\n",
    "    hypotheses = entry[\"hypotheses\"].tolist()\n",
    "    srcs = [src] * len(hypotheses)\n",
    "    predicted_scores = wrapped_model.get_mean(srcs, hypotheses)\n",
    "\n",
    "    all_mbr_scores.append(mbr_scores)\n",
    "    all_predicted_scores.append(predicted_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we are going to calculate the mean squared error\n",
    "differences = []\n",
    "c = 0\n",
    "for target, predicted in zip(all_mbr_scores, all_predicted_scores):\n",
    "    c += 1\n",
    "    \n",
    "    t = np.array(target)\n",
    "    p = np.array(predicted).flatten()\n",
    "\n",
    "    differences.append((t - p).tolist())\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22110d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we calc the MSE:\n",
    "total_squared_error = 0\n",
    "count = 0\n",
    "\n",
    "squared_errors = []\n",
    "\n",
    "for diff_list in differences:\n",
    "    count += len(diff_list)\n",
    "    \n",
    "    squared_errors.append(np.array(diff_list)**2)\n",
    "    \n",
    "    total_squared_error += np.sum(np.array(diff_list)**2)\n",
    "    \n",
    "print(count)\n",
    "print(total_squared_error)\n",
    "print(total_squared_error/count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1707099a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we plot the squared errors:\n",
    "\n",
    "flattened_squarred_errors = []\n",
    "for e in squared_errors:\n",
    "    flattened_squarred_errors += list(e)\n",
    "\n",
    "print(flattened_squarred_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7856bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(flattened_squarred_errors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54263516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_best_errors = []\n",
    "predicted_best_errors = []\n",
    "\n",
    "# Next we get the error of the top pick compared to predicted one\n",
    "for target, predicted, errors in zip(all_mbr_scores, all_predicted_scores, squared_errors):\n",
    "    best_target_index = np.argmax(target)\n",
    "    best_predicted_index = np.argmax(predicted)\n",
    "    target_best_errors.append(errors[best_target_index])\n",
    "    predicted_best_errors.append(errors[best_predicted_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69490ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(target_best_errors)\n",
    "print(np.mean(target_best_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd894ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(predicted_best_errors)\n",
    "print(np.mean(predicted_best_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a469e00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
