{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b154b965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration de-en-lang1=de,lang2=en\n",
      "Reusing dataset tatoeba (C:\\Users\\gerso\\.cache\\huggingface\\datasets\\tatoeba\\de-en-lang1=de,lang2=en\\0.0.0\\b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d698e89be4f44b5082cf70ce9d2df8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[276634, 20738, 2500, 2500, 5000]\n"
     ]
    }
   ],
   "source": [
    "from utils.config_utils import parse_config\n",
    "parsed_config = parse_config('./configs/helsinki-taboeta-de-en.yml', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0788d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "model =parsed_config[\"model\"].to(\"cuda\")\n",
    "model.eval()\n",
    "tokenizer = parsed_config[\"tokenizer\"]\n",
    "example_sentence = \"wie gehts\"#\"Es ist schwierig, objektiv über Gold zu sprechen\"\n",
    "n_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b9def9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarianMTModel(\n",
      "  (model): MarianModel(\n",
      "    (shared): Embedding(58101, 512, padding_idx=58100)\n",
      "    (encoder): MarianEncoder(\n",
      "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
      "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
      "      (layers): ModuleList(\n",
      "        (0): MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): MarianDecoder(\n",
      "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
      "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
      "      (layers): ModuleList(\n",
      "        (0): MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=58101, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2992a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.translation_model_utils import translate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "965ed429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({\"We're going.\": 32, 'Please go.': 17, \"We're doing.\": 13, 'How are things going.': 13, \"You're going.\": 9, 'Like you go.': 8, \"We're fine.\": 8, \"We'll go.\": 7, \"We're going the way.\": 7, \"We're walking.\": 7, \"There's no way to go.\": 7, \"We're as going.\": 7, \"You're as going.\": 6, \"You're doing.\": 6, 'How are we going.': 6, \"We're going!\": 5, 'As he goes.': 5, \"As you're going.\": 5, \"We're like going.\": 5, \"We're dying.\": 5, \"There's no way.\": 5, \"We're nearly going.\": 5, \"We're leaving.\": 5, 'How are things going?': 4, \"We'll go the way.\": 4, \"We'll go!\": 4, 'How are things going!': 4, 'We go the way.': 4, \"We're doing!\": 4, \"That's how to go.\": 4, \"It's like going.\": 4, 'As you go.': 4, \"We're sinking.\": 3, \"We're How going.\": 3, \"There's nothing like going.\": 3, 'How are things?': 3, \"We're up to things.\": 3, \"There's a way.\": 3, \"There's like going.\": 3, 'Just like done.': 3, 'As it goes.': 3, 'Like it goes.': 3, 'How are you going.': 3, 'As to go.': 3, 'This is how we go.': 2, \"As if you're going.\": 2, \"This is how we're going.\": 2, 'Just like go.': 2, \"You're doing things.\": 2, \"We'll be doing.\": 2, 'Like here.': 2, \"You're joking.\": 2, 'We are going.': 2, \"We're just going.\": 2, 'Here goes.': 2, 'We way.': 2, \"It's like walking.\": 2, 'This is how to go.': 2, 'I feel like going.': 2, \"I don't feel like going.\": 2, 'pushed.': 2, \"You're fine.\": 2, \"We're doing fine.\": 2, \"That's like going.\": 2, 'The way is going.': 2, 'We have to go.': 2, \"We're going How.\": 2, \"That's how we're doing.\": 2, \"Let's go.\": 2, 'We are walking.': 2, \"Like you don't go.\": 2, \"There's a perfectly going.\": 1, \"We'll go there.\": 1, 'program human watches.': 1, 'Like is doing.': 1, \"There's just like going.\": 1, \"That's just like going.\": 1, 'Like we go.': 1, 'Like goes.': 1, \"We're up to here.\": 1, 'We go again.': 1, 'Please were going.': 1, 'That twenty- adored.': 1, \"We're out of the difference.\": 1, \"We'reigt changing thecor.\": 1, 'This is how you go.': 1, 'cakes are like longest.': 1, \"We're that going.\": 1, \"We're studied.\": 1, \"We'll be fine.\": 1, 'We have as it could.': 1, 'As▁sagen goes.': 1, 'detail walks.': 1, \"We'll go do the way.\": 1, 'Your way.': 1, 'The Can you go.': 1, \"We're laid out.\": 1, 'More walks.': 1, \"We're answer the▁würde.\": 1, 'designed the way.': 1, 'Parliaments are going.': 1, 'uses are going.': 1, 'Leave like this.': 1, 'anymore.': 1, \"We're game women.\": 1, \"We're nearly walking.\": 1, 'particularly goes.': 1, \"C's going.\": 1, \"We'rered.\": 1, 'All things go.': 1, 'As Me shall.': 1, 'hundred Go.': 1, \"We're anyway.\": 1, 'Wilsons like done.': 1, 'room affected.': 1, \"fun You're doing it.\": 1, 'bombs like going.': 1, 'protection like this.': 1, 'As you can.': 1, \"You're loves.\": 1, 'Wet you.': 1, 'burnt like dentist.': 1, \"We're super.\": 1, \"powers's going.\": 1, \"We're both going.\": 1, \"We'll be possible to go.\": 1, 'stronger German is going.': 1, \"We're fireing.\": 1, 'The way.': 1, \"We're as walking.\": 1, 'We have quarter to go.': 1, \"We We're walking.\": 1, 'wore to leave.': 1, 'daughterszulegens.': 1, \"We're medieval.\": 1, 'studying like this.': 1, 'C Feeddwa.': 1, 'As is often!': 1, \"We'll do the dishes.\": 1, 'Ast might go.': 1, \"posted like he's going.\": 1, 'teenager walks.': 1, \"There's now as possible.\": 1, 'Like off, go.': 1, 'hWhat is going.': 1, \"As you're going, you'll be doing.\": 1, 'calms like going.': 1, 'It sounds like going.': 1, 'There is no way.': 1, \"There's as much acted as may.\": 1, \"We're going fine.\": 1, 'jungles are going.': 1, 'We are as going.': 1, \"You're losing.\": 1, 'As you criminal go.': 1, 'o comes.': 1, \"As if we're going, go.\": 1, 'gender like girlfriend.': 1, \"Does thee-up's going.\": 1, 'How are official splendid.': 1, '▁Fühl.': 1, \"It looks like you're doing.\": 1, \"That's as▁Größere.\": 1, \"As we are, we'll go.\": 1, \"We're going to the way.\": 1, \"We'lll m 17 tidy.\": 1, 'com asks.': 1, \"We're goingking.\": 1, 'As you are!': 1, 'Nothing is going.': 1, \"Please, as you'll go.\": 1, 'This is how work.': 1, 'We are everywhere to go.': 1, \"That's how you walk.\": 1, 'customer is going.': 1, \"We'll go the whole way.\": 1, \"We're didn't feel like going.\": 1, \"As if we're going.\": 1, 'How become going.': 1, 'thus- Modular.': 1, \"You're it like isn't working.\": 1, 'Whether we go.': 1, \"There's a way to go.\": 1, 'As shall go.': 1, 'Please run.': 1, 'Like it way.': 1, \"We're going as possible.\": 1, \"We're off to the times.\": 1, \"We're Never going!\": 1, \"We'll lead.\": 1, 'Here we go.': 1, \"You're thing.\": 1, \"We're With thesatisfieds.\": 1, \"We're skills.\": 1, \"There's been some way.\": 1, \"Please, as's going.\": 1, \"Like you're perfectly.\": 1, 'difference if we go.': 1, 'studied!': 1, 'As touch, go.': 1, 'activity starting.': 1, 'We are at things deserting.': 1, 'grew the crime.': 1, 'How are things go.': 1, \"It's like welcome.\": 1, \"We're doing it.\": 1, 'Please, How are you doing.': 1, \"We We're going.\": 1, \"We'll just go.\": 1, 'brings.': 1, 'As is.': 1, \"As usual, like there's no way to go.\": 1, 'The way is about to go.': 1, \"That's how will go.\": 1, 'Dog loud.': 1, \"Here's the way.\": 1, \"Luc▁jedes's going.\": 1, 'our how you go.': 1, \"This is as you which're going.\": 1, 'oting like done.': 1, 'description will go.': 1, \"We're like going!\": 1, 'played the kids.': 1, \"We're all going.\": 1, 'regardless like go.': 1, \"We're imitate zero.\": 1, 'Wasback Maria.': 1, 'As there are already alone.': 1, 'Sho- art going.': 1, 'temples are disappointed.': 1, \"You'remoken.\": 1, 'exceeded like forget.': 1, 'An easy way.': 1, 'As with you, please.': 1, 'h jacket goes.': 1, 'rrys go.': 1, \"You're as away.\": 1, 'To go.': 1, 'As if you should go.': 1, \"As if you're going, attack.\": 1, 'Please, as usual.': 1, \"We'resa M reigned.\": 1, 'You will go!': 1, 'Vote will go.': 1, \"We're doing here.\": 1, \"You're like going.\": 1, 'Like you eat.': 1, \"As if you're going, please.\": 1, 'Educations goes.': 1, 'Like is happening.': 1, 'bit right.': 1, '▁wies love!': 1, \"We're class▁gibt.\": 1, 'monster goes.': 1, 'Everything goes.': 1, 'partnerion will go.': 1, \"We're staying like there.\": 1, \"We're raining.\": 1, \"We'rekty.\": 1, 'I have started.': 1, 'Please, like to go.': 1, \"We're going theson.\": 1, 'We are your feet.': 1, 'Canadian such as you go.': 1, \"You're already fine.\": 1, 'Please, like something.': 1, \"We're the technology.\": 1, 'milesck how are going.': 1, \"We're moment help.\": 1, 'How are we going?': 1, 'I do theag.': 1, 'strong will go.': 1, \"We're going as entering.\": 1, 'elephants will go.': 1, \"We'll go with anything.\": 1, 'As we go.': 1, 'twisteds are going.': 1, \"You'll be going.\": 1, 'About the gets.': 1, 'la walks.': 1, 'There are everywheres.': 1, 'Just like do.': 1, 'Asman goes.': 1, 'Please, how are things.': 1, \"We're nearly.\": 1, \"We're going a fire leaving.\": 1, \"We're How serious.\": 1, 'We will walk like that.': 1, 'As do as we go.': 1, 'As if there is ruining.': 1, \"We're going,▁unbekannt.\": 1, \"We're driving.\": 1, 'As smile.': 1, \"We're going again.\": 1, 'fit.': 1, 'We have started.': 1, \"That's as he can walk.\": 1, 'Well be doing.': 1, 'concert is going.': 1, \"Can've gone!\": 1, \"We're spirits.\": 1, 'There goes Please.': 1, \"We're bragging.\": 1, \"We'rede yourself to go.\": 1, 'We are nearly going.': 1, \"As if you're played.\": 1, 'We make up just as possible.': 1, 'hats like Each.': 1, 'As Leave is possible.': 1, \"We'lle the dishes.\": 1, 'E bill.': 1, 'As oderpped is going.': 1, \"test like tore's nearly.\": 1, \"increased's How.\": 1, \"We'll case.\": 1, \"We're going how.\": 1, 'possibly is going.': 1, \"You're staying right.\": 1, \"We're wrestling.\": 1, \"We'reelling.\": 1, \"Please, how we're going.\": 1, '▁Männer how.': 1, 'Like then, go.': 1, \"As is, there's a will.\": 1, \"I'll be along.\": 1, \"We're football search.\": 1, 'ship going the way.': 1, 'As imagine, go.': 1, 'As game begins.': 1, \"There's show up!\": 1, 'similar to cancer.': 1, 'Such things go.': 1, 'played stingingings.': 1, 'We go do it like usual.': 1, 'Chi Fill work.': 1, \"There's answers way.\": 1, 'Please accept.': 1, 'Please make yourself go.': 1, 'perfectly goes.': 1, 'One way to go.': 1, 'points it.': 1, 'nonsenseing.': 1, 'picked up the phone.': 1, \"We'rewater.\": 1, \"There's Hell to go.\": 1, 'We arell walk.': 1, \"How are We're going.\": 1, \"We're going to face.\": 1, \"We'll go as far.\": 1, 'Either goes.': 1, 'Like body.': 1, 'Please, as are.': 1, 'There goes straight.': 1, 'As usual.': 1, 'drinking is as road▁Gesicht.': 1, 'As if we go.': 1, 'repeats will go!': 1, 'Like things go.': 1, \"world's stay.\": 1, 'Please, as he is going.': 1, \"We're stoveing.\": 1, 'possibly are going.': 1, \"We'reiaing.\": 1, \"We're fine!\": 1, 'finals go.': 1, 'Watch it likeo.': 1, 'Something is going.': 1, 'Pull such as done.': 1, 'par how to go.': 1, \"We don't feel like going.\": 1, \"That's just like you're doing.\": 1, 'How are things to go?': 1, \"As if you're going, go.\": 1, 'flood like doing.': 1, \"There's go.\": 1, 'As will go.': 1, \"I'll go, way.\": 1, 'slavery- learning how.': 1, 'cloudy such as go.': 1, \"We're lost.\": 1, \"There's straight.\": 1, 'Your place is as slow as Tom.': 1, \"As we're going.\": 1, \"As if we don't go!\": 1, 'Dogs like going.': 1, \"Here's how to go.\": 1, 'Will paper.': 1, \"You're horror.\": 1, \"dishes don't go!\": 1, \"We're citizens.\": 1, 'As if you go, go.': 1, 'How goes things.': 1, 'Please will go.': 1, 'We are doing.': 1, 'We do the way.': 1, \"We can't help this way.\": 1, \"There's a increase in the guys's soul.\": 1, 'Like is done.': 1, 'This is how bar.': 1, \"We're almost there.\": 1, 'Did we go.': 1, 'Feel free.': 1, \"We're going a way.\": 1, \"You're doing the▁ganz.\": 1, 'side like doing.': 1, 'hurrys like going.': 1, 'True thing goes.': 1, 'rum- few dulls.': 1, 'pery dwarf beat.': 1, \"We're doing the way.\": 1, 'As far as possible.': 1, \"We're having weird.\": 1, 'Learning is as fast as possible.': 1, \"Can't go.\": 1, \"We're going the running.\": 1, \"We'reoning.\": 1, 'Please, like music.': 1, \"We're sells things.\": 1, 'proposed like doing.': 1, 'cakes like done.': 1, \"We're philosophy.\": 1, 'As is rarely, this is going.': 1, \"We're away.\": 1, 'Please How has been going.': 1, \"You'reggering.\": 1, 'Please everyone goes.': 1, 'du 2013.': 1, 'studying as lot.': 1, 'All▁Geburts are going.': 1, \"There's also to go.\": 1, 'cure for walked.': 1, \"You're entrance to go.\": 1, 'Please magazines.': 1, 'harder go.': 1, 'As do as▁gesucht.': 1, \"We're absolutely going.\": 1, \"We're at night.\": 1, \"You're starting.\": 1, \"We're punished.\": 1, \"We'll walk the way.\": 1, \"suppose you're going.\": 1, 'Please, just as you go.': 1, 'Which way.': 1, 'We always go.': 1, 'Likere starting.': 1, 'rarely goes.': 1, \"We'rere going.\": 1, 'We have lot of way.': 1, \"It's like bedtime.\": 1, 'ess like go.': 1, 'We are legal?': 1, 'We may as well go.': 1, \"We're just the record!\": 1, \"There's as decided to go.\": 1, 'eptic is racking.': 1, \"We're just walking.\": 1, \"We're chuking.\": 1, \"You're fine!\": 1, 'As there things go.': 1, \"I'll go!\": 1, 'We go!': 1, \"There's as much as▁ihrem.\": 1, 'Jews are going.': 1, '▁Maske how are things.': 1, 'Please, like you.': 1, \"As to go, there's a way.\": 1, 'sound like leaves.': 1, 'Such as we go.': 1, 'We will go together.': 1, \"We'redy.\": 1, 'We covered.': 1, 'We can.': 1, 'We are just game.': 1, \"We'll go do the jesu.\": 1, 'Please bring chicken.': 1, 'vel effort▁Küche.': 1, \"It's just like you're going.\": 1, 'I go the way.': 1, 'This is what it is going.': 1, \"We're going happy.\": 1, \"conservative's going.\": 1, 'We can?': 1, 'Please, as we go.': 1, \"We're afraid interrupting.\": 1, \"We're always pa adPlease.\": 1, 'Your cast is on the fus.': 1, \"That's how you go.\": 1, \"We'll have leaving.\": 1, \"We're theow.\": 1, 'boxes like that.': 1, \"We'll do the things.\": 1, \"That's how the direction...\": 1, '▁Soll- Pro takes off.': 1, 'Please, like doing.': 1, 'suit something.': 1, \"We're chose the way.\": 1, 'seldom go.': 1, 'nahm is animal.': 1, 'Pleasell!': 1, \"We'reThey.\": 1, 'How are bagdenken.': 1, 'We go as far.': 1, \"You're welcome.\": 1, 'sets like it.': 1, 'Like isfighter.': 1, 'Like mortal.': 1, 'We are going as 10.': 1, 'usual will go.': 1, \"We're doing the How.\": 1, \"We're filled.\": 1, 'We areka idle.': 1, 'We are pointsing.': 1, \"We're going do.\": 1, 'Asre can go.': 1, \"That'll go.\": 1, 'Ass go.': 1, \"We're house consequently.\": 1, 'We unable to do it.': 1, \"There's a way on the back.\": 1, \"We're staying.\": 1, \"You're game.\": 1, 'similar to going.': 1, 'We go able.': 1, \"We're going to order.\": 1, 'Birds like go.': 1, \"We're woods things going.\": 1, 'Dada-▁Kalenderjahr.': 1, 'permission work.': 1, \"We're going do it.\": 1, \"We're traveling.\": 1, 'Aster goes.': 1, \"We're fine with this.\": 1, 'We go things.': 1, 'We may go.': 1, \"You're going like it.\": 1, 'Like speaks.': 1, \"We're acting.\": 1, \"We're going as far.\": 1, \"We can't help going.\": 1, 'As quickly as possible.': 1, 'How are things to be going.': 1, \"You're.\": 1, 'Like▁Betts.': 1, 'We have things to go.': 1, 'This is how you are doing.': 1, 'museum, like going.': 1, \"There's an way to go.\": 1, 'As you go!': 1, 'My t150 are bound.': 1, 'Please just how to go.': 1, 'As you go, go.': 1, 'We have gone.': 1, \"You're doing here.\": 1, \"There's an idea.\": 1, \"You're like done.\": 1, \"We're hanging out.\": 1, \"condition's as contagious.\": 1, \"You're things going.\": 1, \"We're staying at my house.\": 1, 'Therest looks like going.': 1, 'This is how to studies.': 1, \"You're in the way.\": 1, \"We're just like going.\": 1, 'whatever goes.': 1, 'man is going.': 1, \"We're lost!\": 1, 'As if done.': 1, 'singers later.': 1, 'South▁Sonne.': 1, \"You're singing.\": 1, \"That's as we go.\": 1, 'liars are going.': 1, 'We may as you go.': 1, 'disnn is going.': 1, \"We're games.\": 1, 'dating how.': 1, 'As is going.': 1, 'office Our way.': 1, 'We are house Like.': 1, \"That's just how you go.\": 1, 'Please act.': 1, 'Sam unusually goes.': 1, \"We're nearly lost.\": 1, 'We have gone human.': 1, 'My curious is on her way.': 1, \"We're turning around.\": 1, \"We're to go.\": 1, 'We go if there.': 1, \"We'll have to go.\": 1, 'Whatever goes is going.': 1, \"We're It'll go.\": 1, \"We'll be walking.\": 1, \"That's as batteries.\": 1, 'center like doing.': 1, \"We're kilos respect.\": 1, 'foot perfectly goes.': 1, \"body-Can't go.\": 1, 'Was realistic is de Okay.': 1, \"As're piece, go.\": 1, 'This is as funny as to go.': 1, \"We're An kitchen.\": 1, 'We will have lost way to go.': 1, \"We're Younging.\": 1, \"As if you're going, either.\": 1, 'As is worked.': 1, 'situations are going.': 1, 'We have at most.': 1, 'Like it speech.': 1, 'However, things are going.': 1, \"We're fine to go.\": 1, 'Please house what sleeps.': 1, 'danger like going.': 1, 'Good-Why said.': 1, 'Asbed.': 1, \"We'retti.\": 1, \"You're the same way.\": 1, 'ordered like going.': 1, 'Bothgiving goes.': 1, \"We're doing things.\": 1, \"We're along.\": 1, \"You're our thinking.\": 1, 'As you are things.': 1, 'We go show.': 1, 'As if we can go.': 1, 'slept like that.': 1, 'Well have sorry.': 1, 'able to do the crime.': 1, 'We shall our way.': 1, 'As you go, get going.': 1, 'We are sure to go.': 1, 'hotel- China.': 1, \"We'll do the r feel.\": 1, \"We're up welcomed.\": 1, \"We're whiding at just theol.\": 1, 'Like leaving.': 1, 'Could we go?': 1, \"Like you're going.\": 1, 'fed me up.': 1, 'We have fine way.': 1, 'We go let out.': 1, 'As Not?\"': 1, 'How are way.': 1, 'Stand the whole way.': 1, \"We're going, acceptable.\": 1, \"We're insulting.\": 1, 'There are fl lights.': 1, 'bittiert or words.': 1, \"We're divided.\": 1, 'studied at all.': 1, \"We're mailing.\": 1, 'nearby as he goes.': 1, 'We deal.': 1, 'Please, like done.': 1, 'formations as fis sar apart.': 1, \"How's going?\": 1, \"co▁Erstaunlichsman' eyes.\": 1, 'We can get along.': 1, 'team goes.': 1, 'Give like played.': 1, 'We will walk.': 1, \"We're boxing.\": 1, \"We're record it.\": 1, \"While thing goes, Please let's go.\": 1, 'How are?': 1, 'Will go.': 1, 'Like at once.': 1, 'chin- Manda drinks.': 1, 'My way of doing.': 1, 'Pass thezz.': 1, \"We're achieve.\": 1, 'deceived- Street games.': 1, 'Please, as to go.': 1, \"We can't have been easy.\": 1, 'As Do as possible.': 1, \"princess's walking.\": 1, 'We have finished.': 1, 'As seems to go.': 1, 'As if there are things, How.': 1, 'We will.': 1, 'vitaminssolves.': 1, \"You're as fine as to go.\": 1, \"As if you're starting.\": 1, \"Please just like I'm going.\": 1, 'seen how.': 1, 'As if you go.': 1, 'fixing like done.': 1, 'This is like going.': 1, 'embarrassed as to go.': 1, 'We go our way.': 1, 'Even like going.': 1, \"We're repaired.\": 1, \"As if there's done.\": 1, 'save how angry go.': 1, \"You're doing this.\": 1, \"As grow' didn't go.\": 1, \"We're scared.\": 1, \"We're going arrest.\": 1, \"We're going like things.\": 1, 'Asre Income goes.': 1, 'How are we doing?': 1, \"There's your way.\": 1, 'As fast as ever.': 1, \"We'll the way.\": 1, \"We're as abandon.\": 1, \"That's what's going.\": 1, \"That's how we're going.\": 1, 'Please, as possible.': 1, \"We're completely going.\": 1, 'duty is as Italy.': 1, 'lean like that.': 1, 'I go as we can.': 1, 'negotiate such as you go.': 1, 'As is to go.': 1, \"We're▁Feuer.\": 1, 'Like dark.': 1, 'competitionion.': 1, 'writing is as▁Kiste.': 1, 'Rom danger went!': 1, 'ne thought perfectly.': 1, \"We're going▁vermisses.\": 1, 'I do go.': 1, 'Likes going.': 1, \"As if you're starting, let's.\": 1, \"We're game.\": 1, \"We're Pop.\": 1, 'What goes is like.': 1, 'saying survive.': 1, \"We're netasche stood.\": 1, 'Our teacher, like a game.': 1, \"You're streets.\": 1, \"You'reed.\": 1, 'seldom belt.': 1, 'peek is as sacking.': 1, \"As you're doing.\": 1, 'Like is go.': 1, 'We will go.': 1, 'As you will go.': 1, \"We'll have walked.\": 1, 'It is like going.': 1, \"We're Won't.\": 1, 'Tom speed will do.': 1, 'Almost the belong.': 1, \"We're going eager.\": 1, \"Ju Researche's going.\": 1, \"We'll do the possible way.\": 1, \"We're interviewing.\": 1, \"As if we're put inside, get going.\": 1, 'As usual, go.': 1, 'As is done.': 1, \"There's been like going.\": 1, 'vegetables like going.': 1, 'Way to go.': 1})\n",
      "--- 8.855000019073486 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Measure running time of \n",
    "start_time = time.time()\n",
    "example_sentence_repeated = [example_sentence] *1000\n",
    "translations = translate(model, tokenizer, example_sentence_repeated, batch_size=8,)\n",
    "print(Counter(translations))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24dd9253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_samples = tokenizer(example_sentence, return_tensors=\"pt\").to(\"cuda\")\n",
    "# start_time = time.time()\n",
    "\n",
    "# sample = model.generate(\n",
    "#             **tokenized_samples,\n",
    "#             do_sample=True,\n",
    "#             top_k=0,\n",
    "#             temperature=1.0,\n",
    "#             output_scores=True,\n",
    "#             return_dict_in_generate=True,\n",
    "#             num_beams=1\n",
    "    \n",
    "#         )\n",
    "# print(sample)\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812ba7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will try to speed this up.\n",
    "# First we will wrap the model\n",
    "\n",
    "\n",
    "\n",
    "class ModelWrapperForFastSampling:\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        \n",
    "        self.encoder = model.model.encoder\n",
    "        self.decoder = model.model.decoder\n",
    "        self.head = model.lm_head\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "\n",
    "    \n",
    "    def generate_samples(self, sentence, n_samples=1000):\n",
    "        print(dir(self.model))\n",
    "    \n",
    "        possible_tokens = np.arange(self.tokenizer.vocab_size)\n",
    "        start_token = self.tokenizer.pad_token_id\n",
    "        print(start_token)\n",
    "        with torch.no_grad():\n",
    "            tokenized_samples = self.tokenizer(sentence, return_tensors=\"pt\").to(\"cuda\")\n",
    "            print(tokenized_samples)\n",
    "            \n",
    "            temp = self.model.forward(**tokenized_samples, decoder_input_ids=torch.tensor([[start_token]]).to(\"cuda\"))\n",
    "            temp_logits = temp[\"logits\"]\n",
    "            \n",
    "            start_time = time.time()\n",
    "            hidden_state = self.encoder.forward(**tokenized_samples)[\"last_hidden_state\"]\n",
    "            tokens = torch.Tensor([[start_token]]).to(\"cuda\").long()\n",
    "        \n",
    "            start_time = time.time()\n",
    "            out = self.decoder(tokens, encoder_hidden_states=hidden_state)\n",
    "            \n",
    "            logits = self.head(out.last_hidden_state)\n",
    "            probs = torch.softmax(logits, dim=-1, ).cpu().flatten().numpy()\n",
    "            # Normalize\n",
    "            print(np.max(probs))\n",
    "\n",
    "            samples = np.random.choice(len(probs),  size=n_samples, p=probs,replace=True)\n",
    "            print(Counter(samples))\n",
    "            \n",
    "            print(torch.all(logits == temp_logits))\n",
    "            print(logits.shape)\n",
    "            logits[:,:, self.tokenizer.pad_token_id] = float(\"-inf\")\n",
    "            print(\"bias\")\n",
    "            print(torch.max(model.final_logits_bias))\n",
    "            print(model.final_logits_bias)\n",
    "            probs = torch.softmax(logits, dim=-1,).cpu().flatten().numpy()\n",
    "            print(np.max(probs))\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91a77e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_compatibility_gradient_checkpointing', '_backward_hooks', '_buffers', '_call_impl', '_convert_head_mask_to_5d', '_create_or_get_repo', '_expand_inputs_for_generation', '_forward_hooks', '_forward_pre_hooks', '_from_config', '_get_backward_hooks', '_get_decoder_start_token_id', '_get_logits_processor', '_get_logits_warper', '_get_name', '_get_pad_token_id', '_get_repo_url_from_name', '_get_resized_embeddings', '_get_resized_lm_head', '_get_stopping_criteria', '_hook_rss_memory_post_forward', '_hook_rss_memory_pre_forward', '_init_weights', '_is_full_backward_hook', '_keys_to_ignore_on_load_missing', '_keys_to_ignore_on_load_unexpected', '_keys_to_ignore_on_save', '_load_from_state_dict', '_load_state_dict_into_model', '_load_state_dict_into_model_low_mem', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_prepare_attention_mask_for_generation', '_prepare_decoder_input_ids_for_generation', '_prepare_encoder_decoder_kwargs_for_generation', '_prepare_input_ids_for_generation', '_push_to_hub', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_reorder_cache', '_replicate_for_data_parallel', '_resize_final_logits_bias', '_resize_token_embeddings', '_save_to_state_dict', '_set_default_torch_dtype', '_set_gradient_checkpointing', '_slow_forward', '_state_dict_hooks', '_tie_encoder_decoder_weights', '_tie_or_clone_weights', '_update_model_kwargs_for_generation', '_version', 'add_memory_hooks', 'add_module', 'adjust_logits_during_generation', 'apply', 'base_model', 'base_model_prefix', 'beam_sample', 'beam_search', 'bfloat16', 'buffers', 'children', 'config', 'config_class', 'cpu', 'cuda', 'device', 'double', 'dtype', 'dummy_inputs', 'dump_patches', 'estimate_tokens', 'eval', 'extra_repr', 'final_logits_bias', 'float', 'floating_point_ops', 'forward', 'framework', 'from_pretrained', 'generate', 'get_buffer', 'get_decoder', 'get_encoder', 'get_extended_attention_mask', 'get_extra_state', 'get_head_mask', 'get_input_embeddings', 'get_output_embeddings', 'get_parameter', 'get_position_embeddings', 'get_submodule', 'gradient_checkpointing_disable', 'gradient_checkpointing_enable', 'greedy_search', 'group_beam_search', 'half', 'init_weights', 'invert_attention_mask', 'is_gradient_checkpointing', 'is_parallelizable', 'lm_head', 'load_state_dict', 'model', 'modules', 'name_or_path', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_parameters', 'parameters', 'post_init', 'prepare_decoder_input_ids_from_labels', 'prepare_inputs_for_generation', 'prune_heads', 'push_to_hub', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_parameter', 'requires_grad_', 'reset_memory_hooks_state', 'resize_position_embeddings', 'resize_token_embeddings', 'retrieve_modules_from_names', 'sample', 'save_pretrained', 'set_extra_state', 'set_input_embeddings', 'set_output_embeddings', 'share_memory', 'state_dict', 'supports_gradient_checkpointing', 'tie_weights', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
      "58100\n",
      "{'input_ids': tensor([[107, 652,   6,   0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1]], device='cuda:0')}\n",
      "0.36841977\n",
      "Counter({157: 390, 432: 91, 441: 57, 650: 48, 146: 47, 1140: 41, 5424: 28, 487: 24, 140: 18, 38: 13, 36: 9, 131: 9, 1913: 8, 4151: 6, 2984: 5, 1632: 5, 968: 4, 2320: 3, 93: 3, 399: 3, 1663: 3, 7230: 3, 778: 3, 1049: 2, 17296: 2, 8089: 2, 272: 2, 702: 2, 3748: 2, 11230: 2, 37108: 1, 2358: 1, 40818: 1, 789: 1, 17903: 1, 877: 1, 858: 1, 12644: 1, 31911: 1, 12971: 1, 13266: 1, 10721: 1, 1117: 1, 3555: 1, 3994: 1, 14339: 1, 958: 1, 1297: 1, 1472: 1, 1002: 1, 12578: 1, 1327: 1, 19294: 1, 2588: 1, 27401: 1, 1266: 1, 34829: 1, 2739: 1, 24851: 1, 6924: 1, 9076: 1, 16883: 1, 8457: 1, 29869: 1, 352: 1, 35325: 1, 5467: 1, 6011: 1, 25043: 1, 21302: 1, 20068: 1, 3372: 1, 4186: 1, 231: 1, 14670: 1, 2380: 1, 2432: 1, 1328: 1, 3440: 1, 14192: 1, 14873: 1, 22963: 1, 1343: 1, 8511: 1, 28750: 1, 48488: 1, 27418: 1, 11865: 1, 31886: 1, 47015: 1, 37991: 1, 6923: 1, 23636: 1, 7256: 1, 2847: 1, 31400: 1, 7967: 1, 13521: 1, 5191: 1, 4825: 1, 587: 1, 736: 1, 47: 1, 25337: 1, 282: 1, 4882: 1, 4820: 1, 6556: 1, 15631: 1, 8487: 1, 3068: 1, 29829: 1, 20235: 1, 8322: 1, 31269: 1, 7078: 1, 36955: 1, 5714: 1, 429: 1, 23837: 1, 6874: 1, 13673: 1, 6964: 1, 10415: 1, 2721: 1, 1127: 1, 23757: 1, 4891: 1, 444: 1, 13232: 1, 5873: 1, 69: 1, 27623: 1, 746: 1, 13159: 1, 64: 1, 14590: 1, 15718: 1, 37949: 1, 28445: 1, 3492: 1, 20425: 1, 21154: 1, 38478: 1, 7721: 1, 1301: 1, 14504: 1, 389: 1, 16083: 1, 8272: 1, 682: 1, 4470: 1, 1795: 1, 8375: 1, 3706: 1, 23367: 1, 25946: 1, 23544: 1, 12325: 1, 5892: 1, 1658: 1, 3030: 1, 18828: 1, 6731: 1, 209: 1, 7350: 1, 15319: 1, 1007: 1, 33457: 1, 36279: 1, 414: 1, 45629: 1, 9718: 1, 12918: 1, 3799: 1, 173: 1, 9309: 1, 7535: 1, 45185: 1, 3517: 1, 7094: 1, 17384: 1, 41394: 1, 12782: 1, 11321: 1, 4077: 1, 4266: 1, 21957: 1, 12691: 1, 12936: 1, 3300: 1, 142: 1, 18151: 1, 2632: 1, 23676: 1})\n",
      "tensor(True, device='cuda:0')\n",
      "torch.Size([1, 1, 58101])\n",
      "bias\n",
      "tensor(0., device='cuda:0')\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "0.36841977\n"
     ]
    }
   ],
   "source": [
    "wrapped_model = ModelWrapperForFastSampling(model, tokenizer)\n",
    "\n",
    "wrapped_model.generate_samples(example_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c8d82ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aba1a3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58100\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4061905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
